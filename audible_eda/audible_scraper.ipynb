{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.audible.com/search?pf_rd_p=7fe4387b-4762-42a8-8d9a-a63254c74bb2&pf_rd_r=C7ENYKDADHMCH4KY12D4&ref=a_search_l1_feature_five_browse-bin_6&feature_six_browse-bin=9178177011&pageSize=50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_dict(items, category, data):\n",
    "    for item in items:\n",
    "        text_fields = item.text.split('\\n')\n",
    "        link = [link for link in item.absolute_links if '/pd/' in link][0]\n",
    "        dict_entry={\n",
    "            'category' : category,\n",
    "            'title' : text_fields[0],\n",
    "            'link' : link\n",
    "            }\n",
    "        try:\n",
    "            dict_entry['rating_count'] = np.int([s for s in text_fields if 'stars' in s][0].split(\n",
    "                'stars ')[1].replace(',',''))\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['narrator'] = [s for s in text_fields if 'Narrated by' in s][0].split(': ')[1]\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['asin'] = [s for s in link.split('/') if 'B0' in s][0].split('?')[0]\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['length'] = [s for s in text_fields if 'Length' in s][0].split(': ')[1]\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['rating'] = np.float([s for s in text_fields if 'stars' in s][-1].split(' out')[0])\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['author'] = [s for s in text_fields if 'By' in s][0].split(': ')[1]\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['price'] = np.float([s for s in text_fields if 'Regular' in s][0].split('$')[1])\n",
    "        except: pass\n",
    "        try:\n",
    "            dict_entry['release_date'] = datetime.strptime([s for s in text_fields if 'Release date:' in s][0].split(\n",
    "                ': ')[1], '%m-%d-%y')\n",
    "        except : pass\n",
    "        data.append(dict_entry)\n",
    "    return data\n",
    "\n",
    "def scrape_great_courses(mthreads, category, pages, url_list, data):\n",
    "    sess = HTMLSession()\n",
    "    \n",
    "    for url in url_list:\n",
    "        try:\n",
    "            r = sess.get(url)\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                r = sess.get(url)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        items = r.html.find('li.bc-list-item.productListItem', first=False)\n",
    "        \n",
    "        threads = []\n",
    "        for j in range(mthreads):\n",
    "            item_sublist = items[j::mthreads]\n",
    "            t = Thread(target=build_dict, args=(item_sublist, category, data))\n",
    "            threads.append(t)\n",
    "            \n",
    "        [t.start() for t in threads]\n",
    "        [t.join() for t in threads]\n",
    "    \n",
    "    sess.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_threader(nthreads, mthreads, category, pages, base_url, data=None):\n",
    "    if data == None:\n",
    "        data = []\n",
    "    \n",
    "    # Create url list\n",
    "    url_list = []\n",
    "    for page in range(pages):\n",
    "        pageurl = base_url + '&page=' + str(page+1)\n",
    "        url_list.append(pageurl)\n",
    "        \n",
    "    # Create threads\n",
    "    threads = []\n",
    "    for i in range(nthreads):\n",
    "        url_sublist = url_list[i::nthreads]\n",
    "        t = Thread(target=scrape_great_courses, args=(mthreads, category, pages, url_sublist, data))\n",
    "        threads.append(t)\n",
    "    \n",
    "    # Run threads\n",
    "    [t.start() for t in threads]\n",
    "    [t.join() for t in threads]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loop_categories(nthreads, mthreads, cat_names, cat_page_nums, cat_links):\n",
    "    data = []\n",
    "    \n",
    "#     widgets = [\n",
    "#         progressbar.Percentage(), \n",
    "#         progressbar.Bar(), \n",
    "#         progressbar.ETA(),\n",
    "#         progressbar.DynamicMessage('cat')]\n",
    "#     bar = progressbar.ProgressBar(widgets=widgets, max_value=sum(cat_page_nums)).start()\n",
    "    \n",
    "    finished_pages = 0  \n",
    "    for category, pages, link in zip(cat_names, cat_page_nums, cat_links):\n",
    "        print('Scraping ', category, '...')\n",
    "#         bar.update(finished_pages, cat=category)\n",
    "        data.extend(scrape_threader(nthreads, mthreads, category, pages, link, data=data))\n",
    "        finished_pages += pages\n",
    "        \n",
    "#     bar.finish()    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = HTMLSession()\n",
    "r = sess.get(base_url)\n",
    "\n",
    "cat_items = r.html.find('div.bc-col-responsive.bc-col-3')[1].find('ul.bc-list')[0].find('li.bc-list-item')\n",
    "cat_names = [item.text.split(' (')[0] for item in cat_items]\n",
    "cat_item_nums = [np.int(item.text.split(' (')[1][:-1].replace(',', '')) for item in cat_items]\n",
    "cat_page_nums = [np.int(np.ceil(item/50)) for item in cat_item_nums]\n",
    "cat_links = [item.absolute_links.pop() + '&pageSize=50' for item in cat_items]\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping  Classics ...\n",
      "Done. Scraped 11042 out of 11043 items at 1.1 pages/s. ETA: 118.2 min.\n",
      "Scraping  Erotica & Sexuality ...\n",
      "Done. Scraped 14405 out of 14405 items at 1.1 pages/s. ETA: 119.0 min.\n",
      "Scraping  Fiction ...\n",
      "Done. Scraped 55564 out of 55614 items at 1.1 pages/s. ETA: 103.4 min.\n",
      "Scraping  History ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3276:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\pyquery\\pyquery.py\", line 95, in fromstring\n",
      "    result = getattr(etree, meth)(context)\n",
      "  File \"src/lxml/etree.pyx\", line 3213, in lxml.etree.fromstring\n",
      "  File \"src/lxml/parser.pxi\", line 1877, in lxml.etree._parseMemoryDocument\n",
      "  File \"src/lxml/parser.pxi\", line 1765, in lxml.etree._parseDoc\n",
      "  File \"src/lxml/parser.pxi\", line 1127, in lxml.etree._BaseParser._parseDoc\n",
      "  File \"src/lxml/parser.pxi\", line 601, in lxml.etree._ParserContext._handleParseResultDoc\n",
      "  File \"src/lxml/parser.pxi\", line 711, in lxml.etree._handleParseResult\n",
      "  File \"src/lxml/parser.pxi\", line 640, in lxml.etree._raiseParseError\n",
      "  File \"<string>\", line 1\n",
      "lxml.etree.XMLSyntaxError: Document is empty, line 1, column 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-9873c939a1ad>\", line 52, in scrape_great_courses\n",
      "    items = r.html.find('li.bc-list-item.productListItem', first=False)\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\requests_html.py\", line 654, in html\n",
      "    self._html = HTML(session=self.session, url=self.url, html=self.content, default_encoding=self.encoding)\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\requests_html.py\", line 421, in __init__\n",
      "    element=PyQuery(html)('html') or PyQuery(f'<html>{html}</html>')('html'),\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\pyquery\\pyquery.py\", line 255, in __init__\n",
      "    elements = fromstring(context, self.parser)\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\pyquery\\pyquery.py\", line 99, in fromstring\n",
      "    result = getattr(lxml.html, meth)(context)\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py\", line 876, in fromstring\n",
      "    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n",
      "  File \"C:\\Users\\Toby-PC\\Anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py\", line 765, in document_fromstring\n",
      "    \"Document is empty\")\n",
      "lxml.etree.ParserError: Document is empty\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Scraped 15500 out of 15621 items at 1.1 pages/s. ETA: 96.7 min.\n",
      "Scraping  Mysteries & Thrillers ...\n",
      "Done. Scraped 47392 out of 47401 items at 1.1 pages/s. ETA: 83.2 min.\n",
      "Scraping  Romance ...\n",
      "Done. Scraped 44607 out of 44623 items at 0.8 pages/s. ETA: 89.2 min.\n",
      "Scraping  Science & Technology ...\n",
      "Done. Scraped 13984 out of 13984 items at 1.1 pages/s. ETA: 62.7 min.\n",
      "Scraping  Sci-Fi & Fantasy ...\n",
      "Done. Scraped 34151 out of 34151 items at 1.1 pages/s. ETA: 56.0 min.\n",
      "Scraping  Self Development ...\n",
      "Done. Scraped 44007 out of 44028 items at 1.1 pages/s. ETA: 39.9 min.\n",
      "Scraping  Comedy ...\n",
      "Done. Scraped 5085 out of 5085 items at 1.2 pages/s. ETA: 36.9 min.\n",
      "Scraping  Newspapers & Magazines ...\n",
      "Done. Scraped 10209 out of 10208 items at 1.2 pages/s. ETA: 32.1 min.\n",
      "Scraping  Nostalgia Radio ...\n",
      "Done. Scraped 2053 out of 2104 items at 1.2 pages/s. ETA: 33.1 min.\n",
      "Scraping  Radio & TV ...\n",
      "Done. Scraped 10697 out of 10697 items at 1.2 pages/s. ETA: 29.3 min.\n",
      "Scraping  Sports ...\n",
      "Done. Scraped 3540 out of 3540 items at 1.1 pages/s. ETA: 30.0 min.\n",
      "Scraping  Travel & Adventure ...\n",
      "Done. Scraped 3526 out of 3526 items at 1.2 pages/s. ETA: 28.7 min.\n",
      "Scraping  Religion & Spirituality ...\n",
      "Done. Scraped 20785 out of 20785 items at 1.1 pages/s. ETA: 22.9 min.\n",
      "Scraping  Nonfiction ...\n",
      "Done. Scraped 17573 out of 17573 items at 1.1 pages/s. ETA: 18.1 min.\n",
      "Scraping  Live Events ...\n",
      "Done. Scraped 1071 out of 1071 items at 1.1 pages/s. ETA: 17.7 min.\n",
      "Scraping  Language Instruction ...\n",
      "Done. Scraped 4454 out of 4454 items at 1.2 pages/s. ETA: 15.9 min.\n",
      "Scraping  Drama & Poetry ...\n",
      "Done. Scraped 3512 out of 3512 items at 1.2 pages/s. ETA: 15.0 min.\n",
      "Scraping  Health & Fitness ...\n",
      "Done. Scraped 7899 out of 7899 items at 1.1 pages/s. ETA: 12.9 min.\n",
      "Scraping  Kids ...\n",
      "Done. Scraped 26689 out of 26689 items at 1.2 pages/s. ETA: 5.0 min.\n",
      "Scraping  Teens ...\n",
      "Done. Scraped 17539 out of 17539 items at 1.1 pages/s. ETA: 0.0 min.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, len(cat_names)):\n",
    "    start=time.time()\n",
    "    df = pd.DataFrame(data=loop_categories(\n",
    "        8, 2, [cat_names[i]], [cat_page_nums[i]], [cat_links[i]]))\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv('{}.csv'.format(i))\n",
    "    end = time.time()\n",
    "    rate = cat_page_nums[i]/(end-start)\n",
    "    pages_left = np.sum(cat_page_nums[i+1:])\n",
    "    eta = pages_left/rate/60\n",
    "    print('Done. Scraped {} out of {} items at {:.1f} pages/s. ETA: {:.1f} min.'.format(\n",
    "        len(df), cat_item_nums[i], rate, eta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0.csv')\n",
    "for i in range(1, len(cat_names)):\n",
    "    df = pd.concat([df, pd.read_csv('{}.csv'.format(i))], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_english_audible.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
